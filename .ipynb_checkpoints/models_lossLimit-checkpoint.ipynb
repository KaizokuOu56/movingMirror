{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e13e0e1",
   "metadata": {},
   "source": [
    "# Fourier Transform using neural networks\n",
    "\n",
    "-   First we generated a dataset of 2000 input functions of various types (2nd cell)\n",
    "-   $80:20$ split of train-test data\n",
    "-   Every function is defined on the interval $(-1, 1)$. Number of points chosen $N = 512$\n",
    "-   The following neural networks are chosen\n",
    "    -   Layers: 3, Activation: RELU\n",
    "    -   Layers: 4, Activation: RELU\n",
    "    -   Layers: 3, Activation: Tanh\n",
    "    -   Layers: 4, Activation: Tanh\n",
    "    -   Layers: 3, Activation: Sigmoid\n",
    "    -   Layers: 4, Activation: Sigmoid\n",
    "-   Training and Test Losses are calculated in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56d8dc72-ad5d-4cfd-a8aa-1778f65434d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import legendre, chebyt, jv, hermite, eval_laguerre\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ff5d575",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_types = [\n",
    "    \"fourier\",          # sum of sinusoids\n",
    "    \"poly\",             # standard polynomial\n",
    "    \"gaussian\",         # single Gaussian\n",
    "    \"gaussian_mixture\", # sum of Gaussians\n",
    "    \"damped_sine\",      # exponentially damped sinusoid\n",
    "    \"exp_decay\",        # exponential decay\n",
    "    \"piecewise\",        # piecewise linear/quadratic\n",
    "    \"trig_combo\",       # combination of sin and cos\n",
    "    \"legendre\",         # Legendre polynomials\n",
    "    \"chebyshev\",        # Chebyshev polynomials (1st kind)\n",
    "    \"bessel\",           # Bessel function of first kind\n",
    "    \"hermite\",          # Hermite polynomials\n",
    "    \"laguerre\",         # Laguerre polynomials\n",
    "    \"windowed_sine\",    # sinusoid multiplied by Gaussian\n",
    "    \"rect_pulse\",       # rectangular pulse\n",
    "    \"sawtooth\",         # sawtooth wave\n",
    "    \"triangle\",         # triangle wave\n",
    "    \"modulated\",        # product of sinusoids (beats)\n",
    "    \"chirp\",            # frequency-increasing sinusoid\n",
    "    \"spikes\",           # sparse impulses\n",
    "    \"wavelet\"           # Mexican hat wavelet\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9175fda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_function(x, kind=\"random\", max_freq=10):\n",
    "    \"\"\"\n",
    "    Generate a diverse set of functions for Fourier testing.\n",
    "\n",
    "    Parameters:\n",
    "        x        : array of input points\n",
    "        kind     : type of function to generate; if 'random', one is picked randomly\n",
    "        max_freq : maximum frequency for Fourier-type functions\n",
    "\n",
    "    Returns:\n",
    "        f        : array of function values\n",
    "    \"\"\"\n",
    "    \n",
    "    if kind == \"random\":\n",
    "        kind = np.random.choice(function_types)\n",
    "    \n",
    "    # ----------------- Standard types -----------------\n",
    "    if kind == \"fourier\":\n",
    "        coeffs = np.random.randn(max_freq)\n",
    "        f = np.zeros_like(x, dtype=float)\n",
    "        for n, a in enumerate(coeffs, start=1):\n",
    "            f += a * np.sin(np.pi * n * x)\n",
    "        return f\n",
    "\n",
    "    elif kind == \"poly\":\n",
    "        coeffs = np.random.randn(5)\n",
    "        return sum(c * x**i for i, c in enumerate(coeffs))\n",
    "\n",
    "    elif kind == \"gaussian\":\n",
    "        mu, sigma = np.random.uniform(-0.5, 0.5), np.random.uniform(0.05, 0.5)\n",
    "        return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))\n",
    "\n",
    "    elif kind == \"gaussian_mixture\":\n",
    "        num_gaussians = np.random.randint(2, 4)\n",
    "        f = np.zeros_like(x)\n",
    "        for _ in range(num_gaussians):\n",
    "            mu, sigma, amp = np.random.uniform(-0.5,0.5), np.random.uniform(0.05,0.3), np.random.uniform(0.5,2.0)\n",
    "            f += amp * np.exp(-((x - mu)**2) / (2 * sigma**2))\n",
    "        return f\n",
    "\n",
    "    elif kind == \"damped_sine\":\n",
    "        freq = np.random.uniform(1, max_freq)\n",
    "        decay = np.random.uniform(0.5, 2.0)\n",
    "        phase = np.random.uniform(0, 2*np.pi)\n",
    "        return np.exp(-decay * np.abs(x)) * np.sin(2 * np.pi * freq * x + phase)\n",
    "\n",
    "    elif kind == \"exp_decay\":\n",
    "        lam = np.random.uniform(0.5, 2.0)\n",
    "        return np.exp(-lam * np.abs(x))\n",
    "\n",
    "    elif kind == \"piecewise\":\n",
    "        split = np.random.uniform(x[0], x[-1])\n",
    "        return np.piecewise(x, [x < split, x >= split],\n",
    "                            [lambda t: t**2, lambda t: -t + split])\n",
    "\n",
    "    elif kind == \"trig_combo\":\n",
    "        f = np.zeros_like(x)\n",
    "        num_terms = np.random.randint(2, 5)\n",
    "        for _ in range(num_terms):\n",
    "            amp = np.random.uniform(0.5, 2.0)\n",
    "            freq = np.random.randint(1, max_freq)\n",
    "            phase = np.random.uniform(0, 2*np.pi)\n",
    "            f += amp * (np.sin(2*np.pi*freq*x + phase) + np.cos(2*np.pi*freq*x + phase))\n",
    "        return f\n",
    "\n",
    "    # ----------------- Special polynomials -----------------\n",
    "    elif kind == \"legendre\":\n",
    "        deg = np.random.randint(1, 6)\n",
    "        P = legendre(deg)\n",
    "        return P(x)\n",
    "\n",
    "    elif kind == \"chebyshev\":\n",
    "        deg = np.random.randint(1, 6)\n",
    "        T = chebyt(deg)\n",
    "        return T(x)\n",
    "\n",
    "    elif kind == \"bessel\":\n",
    "        order = np.random.randint(0, 6)\n",
    "        k = np.random.uniform(1, 10)\n",
    "        return jv(order, k * x)\n",
    "\n",
    "    elif kind == \"hermite\":\n",
    "        deg = np.random.randint(1,5)\n",
    "        H = hermite(deg)\n",
    "        return H(x)\n",
    "\n",
    "    elif kind == \"laguerre\":\n",
    "        deg = np.random.randint(1,5)\n",
    "        return eval_laguerre(deg, np.abs(x))  # Laguerre defined on [0,âˆž)\n",
    "\n",
    "    # ----------------- Windowed / localized functions -----------------\n",
    "    elif kind == \"windowed_sine\":\n",
    "        freq = np.random.uniform(1, max_freq)\n",
    "        alpha = np.random.uniform(1,5)\n",
    "        return np.sin(2*np.pi*freq*x) * np.exp(-alpha*x**2)\n",
    "\n",
    "    elif kind == \"rect_pulse\":\n",
    "        start, end = np.random.uniform(-0.5, 0), np.random.uniform(0,0.5)\n",
    "        return np.where((x>=start) & (x<=end), 1.0, 0.0)\n",
    "\n",
    "    elif kind == \"sawtooth\":\n",
    "        return 2*(x - np.floor(x + 0.5))  # normalized sawtooth\n",
    "\n",
    "    elif kind == \"triangle\":\n",
    "        return 2*np.abs(2*(x - np.floor(x + 0.5))) - 1\n",
    "\n",
    "    elif kind == \"modulated\":\n",
    "        f1 = np.sin(5*np.pi*x)\n",
    "        f2 = np.cos(2*np.pi*x)\n",
    "        return f1*f2\n",
    "\n",
    "    elif kind == \"chirp\":\n",
    "        return np.sin(2*np.pi*(x + x**2))\n",
    "\n",
    "    elif kind == \"spikes\":\n",
    "        f = np.zeros_like(x)\n",
    "        num_spikes = np.random.randint(3,8)\n",
    "        indices = np.random.choice(len(x), num_spikes, replace=False)\n",
    "        f[indices] = np.random.uniform(1,3, size=num_spikes)\n",
    "        return f\n",
    "\n",
    "    elif kind == \"wavelet\":\n",
    "        return (1 - x**2) * np.exp(-x**2 / 2)  # Mexican hat\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown function type '{kind}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecfc9931-b45c-4748-9f32-33b0cdc82dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(function_name, N):\n",
    "    # Load CSV (skip header)\n",
    "    table = np.loadtxt(function_name, delimiter=\",\", skiprows=2)\n",
    "    \n",
    "    num_samples = 2500\n",
    "    \n",
    "    # Split columns\n",
    "    f_flat = table[:,0]\n",
    "    Re_flat = table[:,1]\n",
    "    Im_flat = table[:,2]\n",
    "    \n",
    "    # Reshape into original shapes\n",
    "    f_data = f_flat.reshape(num_samples, N)\n",
    "    F_data = np.stack([Re_flat.reshape(num_samples, N),\n",
    "                       Im_flat.reshape(num_samples, N)], axis=-1)  # shape (num_samples, N, 2)\n",
    "    \n",
    "    return f_data, F_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fb81a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loaders(f_data, F_data):\n",
    "    train_frac = 0.8\n",
    "    split_idx = int(train_frac * len(f_data))\n",
    "    \n",
    "    f_train, f_test = f_data[:split_idx], f_data[split_idx:]\n",
    "    F_train, F_test = F_data[:split_idx], F_data[split_idx:]\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    batch_size = 32\n",
    "    train_ds = TensorDataset(torch.tensor(f_train, dtype = torch.float32), torch.tensor(F_train, dtype = torch.float32))\n",
    "    test_ds = TensorDataset(torch.tensor(f_test, dtype = torch.float32), torch.tensor(F_test, dtype = torch.float32))\n",
    "    train_loader = DataLoader(train_ds, batch_size = batch_size, shuffle = True)\n",
    "    test_loader = DataLoader(test_ds, batch_size = batch_size)\n",
    "\n",
    "    return train_ds, test_ds, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21d6f3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourierNet(nn.Module):\n",
    "    def __init__(self, N, layers=3, activation='ReLU', prelu_alpha=0.01):\n",
    "        \"\"\"\n",
    "        N           : input/output size\n",
    "        layers      : number of hidden layers\n",
    "        activation  : 'ReLU', 'LeakyReLU', 'PReLU', 'Sigmoid', 'Tanh'\n",
    "        prelu_alpha : negative slope for PReLU (if used)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.layers = layers\n",
    "        self.activation = activation\n",
    "        self.prelu_alpha = prelu_alpha\n",
    "\n",
    "        self.model = self._build_model()\n",
    "        self.name = f\"FourierNet(Layers: {self.layers}, Activation: {self.activation})\"\n",
    "\n",
    "    def _build_model(self):\n",
    "        layers = []\n",
    "        in_features = self.N\n",
    "\n",
    "        # First layer\n",
    "        layers.append(nn.Linear(in_features, in_features*2))\n",
    "\n",
    "        for _ in range(self.layers - 1):\n",
    "            # Add activation\n",
    "            if self.activation == 'ReLU':\n",
    "                layers.append(nn.ReLU())\n",
    "            elif self.activation == 'Tanh':\n",
    "                layers.append(nn.Tanh())\n",
    "            elif self.activation == 'Sigmoid':\n",
    "                layers.append(nn.Sigmoid())\n",
    "            elif self.activation == 'LeakyReLU':\n",
    "                layers.append(nn.LeakyReLU(negative_slope=0.01))\n",
    "            elif self.activation == 'PReLU':\n",
    "                layers.append(nn.PReLU(init=self.prelu_alpha))\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown activation: {self.activation}\")\n",
    "\n",
    "            # Add linear layer\n",
    "            layers.append(nn.Linear(in_features*2, in_features*2))\n",
    "            # Note: we keep the layer size constant (in_features*2)\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out.view(-1, self.N, 2)  # reshape to (batch, N, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "73911939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total models created: 60\n"
     ]
    }
   ],
   "source": [
    "# Parameters to sweep\n",
    "Ns = [256, 512, 1024]\n",
    "layers_list = [2, 3, 4, 5]\n",
    "activations = ['ReLU', 'LeakyReLU', 'PReLU', 'Sigmoid', 'Tanh']\n",
    "\n",
    "# Initialize dictionaries\n",
    "models = {}\n",
    "metrics = {}\n",
    "\n",
    "# Loop through all combinations\n",
    "for N in Ns:\n",
    "    for num_layers in layers_list:\n",
    "        for act in activations:\n",
    "            # Create a descriptive name\n",
    "            model_name = f\"N{N}_L{num_layers}_{act}\"\n",
    "            \n",
    "            # Instantiate the FourierNet\n",
    "            model = FourierNet(N=N, layers=num_layers, activation=act)\n",
    "            \n",
    "            # Store in dictionaries\n",
    "            models[model_name] = model\n",
    "            metrics[model_name] = {\"Train Loss\": None, \"Test Loss\": None, \"Training Time\": None, \"Epochs\": None}\n",
    "\n",
    "# Check number of models\n",
    "print(f\"Total models created: {len(models)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "778d4c77-55a1-40ea-8557-eea03e8c1b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_dataset(filename, N):\n",
    "    \"\"\"\n",
    "    Reads a CSV file saved in the format:\n",
    "    First row: header containing N and domain (ignored here, but could parse)\n",
    "    Remaining rows: columns: f(x), Re, Im\n",
    "    Returns:\n",
    "        f_data: shape (num_samples, N)\n",
    "        F_data: shape (num_samples, N, 2)\n",
    "    \"\"\"\n",
    "    # Load table skipping first row (header)\n",
    "    table = np.loadtxt(filename, delimiter=\",\", skiprows=2)\n",
    "    \n",
    "    f_flat = table[:,0]\n",
    "    Re_flat = table[:,1]\n",
    "    Im_flat = table[:,2]\n",
    "\n",
    "    num_samples = 2500\n",
    "    f_data = f_flat.reshape(num_samples, N)\n",
    "    F_data = np.stack([Re_flat.reshape(num_samples, N),\n",
    "                       Im_flat.reshape(num_samples, N)], axis=-1)\n",
    "    \n",
    "    return f_data, F_data\n",
    "\n",
    "# --- Load all datasets ---\n",
    "f_data_256, F_data_256 = load_csv_dataset(\"datasets/discrete_256.csv\", 256)\n",
    "f_data_512, F_data_512 = load_csv_dataset(\"datasets/discrete_512.csv\", 512)\n",
    "f_data_1024, F_data_1024 = load_csv_dataset(\"datasets/discrete_1024.csv\", 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ecb28c9-d2d0-45c3-9e65-3e88d58d58be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, Epoch 5/30, Train Loss=0.003776, Test Loss=0.003714, Elapsed: 0m 1s\n",
      "Early stopping triggered at epoch 6 for N256_L2_ReLU\n",
      "\n",
      "Finished training N256_L2_ReLU, Test Loss=0.001295, Training Time = 0m 1s, Epochs = 6\n",
      "\n",
      "Model 1, Epoch 15/30, Train Loss=0.000109, Test Loss=0.000381, Elapsed: 0m 4s\n",
      "Early stopping triggered at epoch 16 for N256_L2_LeakyReLU\n",
      "\n",
      "Finished training N256_L2_LeakyReLU, Test Loss=0.000312, Training Time = 0m 3s, Epochs = 16\n",
      "\n",
      "Model 2, Epoch 9/30, Train Loss=0.000105, Test Loss=0.000251, Elapsed: 0m 7s\n",
      "Early stopping triggered at epoch 10 for N256_L2_PReLU\n",
      "\n",
      "Finished training N256_L2_PReLU, Test Loss=0.000253, Training Time = 0m 2s, Epochs = 10\n",
      "\n",
      "Model 3, Epoch 5/30, Train Loss=0.000100, Test Loss=0.000155, Elapsed: 0m 8s\n",
      "Early stopping triggered at epoch 6 for N256_L2_Sigmoid\n",
      "\n",
      "Finished training N256_L2_Sigmoid, Test Loss=0.000186, Training Time = 0m 1s, Epochs = 6\n",
      "\n",
      "Model 4, Epoch 9/30, Train Loss=0.000072, Test Loss=0.000148, Elapsed: 0m 10s\n",
      "Early stopping triggered at epoch 10 for N256_L2_Tanh\n",
      "\n",
      "Finished training N256_L2_Tanh, Test Loss=0.000178, Training Time = 0m 2s, Epochs = 10\n",
      "\n",
      "Model 5, Epoch 5/40, Train Loss=0.000087, Test Loss=0.000146, Elapsed: 0m 13s\n",
      "Early stopping triggered at epoch 6 for N256_L3_ReLU\n",
      "\n",
      "Finished training N256_L3_ReLU, Test Loss=0.000126, Training Time = 0m 2s, Epochs = 6\n",
      "\n",
      "Model 6, Epoch 6/40, Train Loss=0.000166, Test Loss=0.000252, Elapsed: 0m 15s\n",
      "Early stopping triggered at epoch 7 for N256_L3_LeakyReLU\n",
      "\n",
      "Finished training N256_L3_LeakyReLU, Test Loss=0.000232, Training Time = 0m 2s, Epochs = 7\n",
      "\n",
      "Model 7, Epoch 6/40, Train Loss=0.000115, Test Loss=0.000250, Elapsed: 0m 17s\n",
      "Early stopping triggered at epoch 7 for N256_L3_PReLU\n",
      "\n",
      "Finished training N256_L3_PReLU, Test Loss=0.000253, Training Time = 0m 2s, Epochs = 7\n",
      "\n",
      "Model 8, Epoch 5/40, Train Loss=0.000038, Test Loss=0.000177, Elapsed: 0m 19s\n",
      "Early stopping triggered at epoch 6 for N256_L3_Sigmoid\n",
      "\n",
      "Finished training N256_L3_Sigmoid, Test Loss=0.000180, Training Time = 0m 2s, Epochs = 6\n",
      "\n",
      "Model 9, Epoch 6/40, Train Loss=0.000327, Test Loss=0.000436, Elapsed: 0m 22s\n",
      "Early stopping triggered at epoch 7 for N256_L3_Tanh\n",
      "\n",
      "Finished training N256_L3_Tanh, Test Loss=0.000512, Training Time = 0m 2s, Epochs = 7\n",
      "\n",
      "Model 10, Epoch 5/60, Train Loss=0.000194, Test Loss=0.000421, Elapsed: 0m 25s\n",
      "Early stopping triggered at epoch 6 for N256_L4_ReLU\n",
      "\n",
      "Finished training N256_L4_ReLU, Test Loss=0.000395, Training Time = 0m 3s, Epochs = 6\n",
      "\n",
      "Model 11, Epoch 5/60, Train Loss=0.000198, Test Loss=0.000236, Elapsed: 0m 28s\n",
      "Early stopping triggered at epoch 6 for N256_L4_LeakyReLU\n",
      "\n",
      "Finished training N256_L4_LeakyReLU, Test Loss=0.000237, Training Time = 0m 2s, Epochs = 6\n",
      "\n",
      "Model 12, Epoch 9/60, Train Loss=0.000217, Test Loss=0.000371, Elapsed: 0m 32s\n",
      "Early stopping triggered at epoch 10 for N256_L4_PReLU\n",
      "\n",
      "Finished training N256_L4_PReLU, Test Loss=0.000284, Training Time = 0m 4s, Epochs = 10\n",
      "\n",
      "Model 13, Epoch 5/60, Train Loss=0.000027, Test Loss=0.000208, Elapsed: 0m 35s\n",
      "Early stopping triggered at epoch 6 for N256_L4_Sigmoid\n",
      "\n",
      "Finished training N256_L4_Sigmoid, Test Loss=0.000212, Training Time = 0m 2s, Epochs = 6\n",
      "\n",
      "Model 14, Epoch 9/60, Train Loss=0.000427, Test Loss=0.000669, Elapsed: 0m 40s\n",
      "Early stopping triggered at epoch 10 for N256_L4_Tanh\n",
      "\n",
      "Finished training N256_L4_Tanh, Test Loss=0.000646, Training Time = 0m 4s, Epochs = 10\n",
      "\n",
      "Model 15, Epoch 12/90, Train Loss=0.000112, Test Loss=0.000378, Elapsed: 0m 48s\n",
      "Early stopping triggered at epoch 13 for N256_L5_ReLU\n",
      "\n",
      "Finished training N256_L5_ReLU, Test Loss=0.000430, Training Time = 0m 8s, Epochs = 13\n",
      "\n",
      "Model 16, Epoch 14/90, Train Loss=0.000076, Test Loss=0.000226, Elapsed: 0m 57s\n",
      "Early stopping triggered at epoch 15 for N256_L5_LeakyReLU\n",
      "\n",
      "Finished training N256_L5_LeakyReLU, Test Loss=0.000264, Training Time = 0m 8s, Epochs = 15\n",
      "\n",
      "Model 17, Epoch 5/90, Train Loss=0.000358, Test Loss=0.000518, Elapsed: 1m 0ss\n",
      "Early stopping triggered at epoch 6 for N256_L5_PReLU\n",
      "\n",
      "Finished training N256_L5_PReLU, Test Loss=0.000727, Training Time = 0m 3s, Epochs = 6\n",
      "\n",
      "Model 18, Epoch 8/90, Train Loss=0.000617, Test Loss=0.000977, Elapsed: 1m 6s\n",
      "Early stopping triggered at epoch 9 for N256_L5_Sigmoid\n",
      "\n",
      "Finished training N256_L5_Sigmoid, Test Loss=0.000947, Training Time = 0m 5s, Epochs = 9\n",
      "\n",
      "Model 19, Epoch 9/90, Train Loss=0.000534, Test Loss=0.000830, Elapsed: 1m 11s\n",
      "Early stopping triggered at epoch 10 for N256_L5_Tanh\n",
      "\n",
      "Finished training N256_L5_Tanh, Test Loss=0.000680, Training Time = 0m 5s, Epochs = 10\n",
      "\n",
      "Model 20, Epoch 38/100, Train Loss=0.000579, Test Loss=0.000836, Elapsed: 1m 42s\n",
      "Early stopping triggered at epoch 39 for N512_L2_ReLU\n",
      "\n",
      "Finished training N512_L2_ReLU, Test Loss=0.000767, Training Time = 0m 30s, Epochs = 39\n",
      "\n",
      "Model 21, Epoch 58/100, Train Loss=0.000951, Test Loss=0.000723, Elapsed: 2m 27s\n",
      "Early stopping triggered at epoch 59 for N512_L2_LeakyReLU\n",
      "\n",
      "Finished training N512_L2_LeakyReLU, Test Loss=0.000449, Training Time = 0m 45s, Epochs = 59\n",
      "\n",
      "Model 22, Epoch 28/100, Train Loss=0.000329, Test Loss=0.000455, Elapsed: 2m 50s\n",
      "Early stopping triggered at epoch 29 for N512_L2_PReLU\n",
      "\n",
      "Finished training N512_L2_PReLU, Test Loss=0.000554, Training Time = 0m 22s, Epochs = 29\n",
      "\n",
      "Model 23, Epoch 14/100, Train Loss=0.000370, Test Loss=0.000533, Elapsed: 3m 1ss\n",
      "Early stopping triggered at epoch 15 for N512_L2_Sigmoid\n",
      "\n",
      "Finished training N512_L2_Sigmoid, Test Loss=0.000478, Training Time = 0m 11s, Epochs = 15\n",
      "\n",
      "Model 24, Epoch 24/100, Train Loss=0.000603, Test Loss=0.000518, Elapsed: 3m 21s\n",
      "Early stopping triggered at epoch 25 for N512_L2_Tanh\n",
      "\n",
      "Finished training N512_L2_Tanh, Test Loss=0.001690, Training Time = 0m 19s, Epochs = 25\n",
      "\n",
      "Model 25, Epoch 14/110, Train Loss=0.000487, Test Loss=0.000592, Elapsed: 3m 42s\n",
      "Early stopping triggered at epoch 15 for N512_L3_ReLU\n",
      "\n",
      "Finished training N512_L3_ReLU, Test Loss=0.000820, Training Time = 0m 22s, Epochs = 15\n",
      "\n",
      "Model 26, Epoch 33/110, Train Loss=0.002483, Test Loss=0.002776, Elapsed: 4m 27s\n",
      "Early stopping triggered at epoch 34 for N512_L3_LeakyReLU\n",
      "\n",
      "Finished training N512_L3_LeakyReLU, Test Loss=0.002446, Training Time = 0m 44s, Epochs = 34\n",
      "\n",
      "Model 27, Epoch 29/110, Train Loss=0.208208, Test Loss=0.003812, Elapsed: 5m 7ss\n",
      "Early stopping triggered at epoch 30 for N512_L3_PReLU\n",
      "\n",
      "Finished training N512_L3_PReLU, Test Loss=0.002909, Training Time = 0m 39s, Epochs = 30\n",
      "\n",
      "Model 28, Epoch 10/110, Train Loss=0.000055, Test Loss=0.000288, Elapsed: 5m 21s\n",
      "Early stopping triggered at epoch 11 for N512_L3_Sigmoid\n",
      "\n",
      "Finished training N512_L3_Sigmoid, Test Loss=0.000286, Training Time = 0m 14s, Epochs = 11\n",
      "\n",
      "Model 29, Epoch 17/110, Train Loss=0.004596, Test Loss=0.002899, Elapsed: 5m 44s\n",
      "Early stopping triggered at epoch 18 for N512_L3_Tanh\n",
      "\n",
      "Finished training N512_L3_Tanh, Test Loss=0.002693, Training Time = 0m 23s, Epochs = 18\n",
      "\n",
      "Model 30, Epoch 12/130, Train Loss=0.000490, Test Loss=0.000671, Elapsed: 6m 9s\n",
      "Early stopping triggered at epoch 13 for N512_L4_ReLU\n",
      "\n",
      "Finished training N512_L4_ReLU, Test Loss=0.000709, Training Time = 0m 26s, Epochs = 13\n",
      "\n",
      "Model 31, Epoch 29/130, Train Loss=0.000277, Test Loss=0.000530, Elapsed: 7m 1ss\n",
      "Early stopping triggered at epoch 30 for N512_L4_LeakyReLU\n",
      "\n",
      "Finished training N512_L4_LeakyReLU, Test Loss=0.000427, Training Time = 0m 51s, Epochs = 30\n",
      "\n",
      "Model 32, Epoch 23/130, Train Loss=0.000303, Test Loss=0.000658, Elapsed: 7m 45s\n",
      "Early stopping triggered at epoch 24 for N512_L4_PReLU\n",
      "\n",
      "Finished training N512_L4_PReLU, Test Loss=0.000612, Training Time = 0m 43s, Epochs = 24\n",
      "\n",
      "Model 33, Epoch 83/130, Train Loss=0.000047, Test Loss=0.000464, Elapsed: 10m 6s\n",
      "Early stopping triggered at epoch 84 for N512_L4_Sigmoid\n",
      "\n",
      "Finished training N512_L4_Sigmoid, Test Loss=0.000434, Training Time = 2m 21s, Epochs = 84\n",
      "\n",
      "Model 34, Epoch 11/130, Train Loss=0.000938, Test Loss=0.001347, Elapsed: 10m 28s\n",
      "Early stopping triggered at epoch 12 for N512_L4_Tanh\n",
      "\n",
      "Finished training N512_L4_Tanh, Test Loss=0.001031, Training Time = 0m 21s, Epochs = 12\n",
      "\n",
      "Model 35, Epoch 25/160, Train Loss=0.000713, Test Loss=0.001071, Elapsed: 11m 34s\n",
      "Early stopping triggered at epoch 26 for N512_L5_ReLU\n",
      "\n",
      "Finished training N512_L5_ReLU, Test Loss=0.000954, Training Time = 1m 6s, Epochs = 26\n",
      "\n",
      "Model 36, Epoch 17/160, Train Loss=0.000652, Test Loss=0.001034, Elapsed: 12m 13s\n",
      "Early stopping triggered at epoch 18 for N512_L5_LeakyReLU\n",
      "\n",
      "Finished training N512_L5_LeakyReLU, Test Loss=0.000973, Training Time = 0m 38s, Epochs = 18\n",
      "\n",
      "Model 37, Epoch 26/160, Train Loss=0.000614, Test Loss=0.000938, Elapsed: 13m 17s\n",
      "Early stopping triggered at epoch 27 for N512_L5_PReLU\n",
      "\n",
      "Finished training N512_L5_PReLU, Test Loss=0.001040, Training Time = 1m 4s, Epochs = 27\n",
      "\n",
      "Model 38, Epoch 10/160, Train Loss=0.002593, Test Loss=0.002901, Elapsed: 13m 42s\n",
      "Early stopping triggered at epoch 11 for N512_L5_Sigmoid\n",
      "\n",
      "Finished training N512_L5_Sigmoid, Test Loss=0.002901, Training Time = 0m 24s, Epochs = 11\n",
      "\n",
      "Model 39, Epoch 25/160, Train Loss=0.000867, Test Loss=0.001101, Elapsed: 14m 39s\n",
      "Early stopping triggered at epoch 26 for N512_L5_Tanh\n",
      "\n",
      "Finished training N512_L5_Tanh, Test Loss=0.001161, Training Time = 0m 57s, Epochs = 26\n",
      "\n",
      "Model 40, Epoch 32/180, Train Loss=0.033304, Test Loss=0.048196, Elapsed: 16m 22ss\n",
      "Early stopping triggered at epoch 33 for N1024_L2_ReLU\n",
      "\n",
      "Finished training N1024_L2_ReLU, Test Loss=0.049234, Training Time = 1m 43s, Epochs = 33\n",
      "\n",
      "Model 41, Epoch 31/180, Train Loss=0.028760, Test Loss=0.098248, Elapsed: 18m 2sss18s\n",
      "Early stopping triggered at epoch 32 for N1024_L2_LeakyReLU\n",
      "\n",
      "Finished training N1024_L2_LeakyReLU, Test Loss=0.173265, Training Time = 1m 39s, Epochs = 32\n",
      "\n",
      "Model 42, Epoch 29/180, Train Loss=0.033232, Test Loss=0.069790, Elapsed: 19m 37sss\n",
      "Early stopping triggered at epoch 30 for N1024_L2_PReLU\n",
      "\n",
      "Finished training N1024_L2_PReLU, Test Loss=0.103254, Training Time = 1m 35s, Epochs = 30\n",
      "\n",
      "Model 43, Epoch 24/180, Train Loss=0.005533, Test Loss=0.003349, Elapsed: 20m 51s\n",
      "Early stopping triggered at epoch 25 for N1024_L2_Sigmoid\n",
      "\n",
      "Finished training N1024_L2_Sigmoid, Test Loss=0.003189, Training Time = 1m 13s, Epochs = 25\n",
      "\n",
      "Model 44, Epoch 70/180, Train Loss=0.045919, Test Loss=0.028741, Elapsed: 24m 24s\n",
      "Early stopping triggered at epoch 71 for N1024_L2_Tanh\n",
      "\n",
      "Finished training N1024_L2_Tanh, Test Loss=0.027082, Training Time = 3m 32s, Epochs = 71\n",
      "\n",
      "Model 45, Epoch 54/190, Train Loss=0.004840, Test Loss=0.004098, Elapsed: 29m 45s\n",
      "Early stopping triggered at epoch 55 for N1024_L3_ReLU\n",
      "\n",
      "Finished training N1024_L3_ReLU, Test Loss=0.004086, Training Time = 5m 24s, Epochs = 55\n",
      "\n",
      "Model 46, Epoch 77/190, Train Loss=0.000298, Test Loss=0.000660, Elapsed: 36m 7ss\n",
      "Early stopping triggered at epoch 78 for N1024_L3_LeakyReLU\n",
      "\n",
      "Finished training N1024_L3_LeakyReLU, Test Loss=0.001499, Training Time = 6m 20s, Epochs = 78\n",
      "\n",
      "Model 47, Epoch 20/190, Train Loss=0.004891, Test Loss=0.004180, Elapsed: 37m 48s\n",
      "Early stopping triggered at epoch 21 for N1024_L3_PReLU\n",
      "\n",
      "Finished training N1024_L3_PReLU, Test Loss=0.004178, Training Time = 1m 41s, Epochs = 21\n",
      "\n",
      "Model 48, Epoch 87/190, Train Loss=0.000101, Test Loss=0.000420, Elapsed: 45m 20s\n",
      "Early stopping triggered at epoch 88 for N1024_L3_Sigmoid\n",
      "\n",
      "Finished training N1024_L3_Sigmoid, Test Loss=0.000418, Training Time = 7m 31s, Epochs = 88\n",
      "\n",
      "Model 49, Epoch 22/190, Train Loss=0.002185, Test Loss=0.004251, Elapsed: 47m 11s\n",
      "Early stopping triggered at epoch 23 for N1024_L3_Tanh\n",
      "\n",
      "Finished training N1024_L3_Tanh, Test Loss=0.004134, Training Time = 1m 51s, Epochs = 23\n",
      "\n",
      "Model 50, Epoch 34/210, Train Loss=0.003852, Test Loss=0.002922, Elapsed: 52m 10s\n",
      "Early stopping triggered at epoch 35 for N1024_L4_ReLU\n",
      "\n",
      "Finished training N1024_L4_ReLU, Test Loss=0.002779, Training Time = 5m 2s, Epochs = 35\n",
      "\n",
      "Model 51, Epoch 85/210, Train Loss=0.736454, Test Loss=1.240458, Elapsed: 61m 44s16ss\n",
      "Early stopping triggered at epoch 86 for N1024_L4_LeakyReLU\n",
      "\n",
      "Finished training N1024_L4_LeakyReLU, Test Loss=1.197322, Training Time = 9m 31s, Epochs = 86\n",
      "\n",
      "Model 52, Epoch 40/210, Train Loss=0.005146, Test Loss=0.004387, Elapsed: 66m 17s9s\n",
      "Early stopping triggered at epoch 41 for N1024_L4_PReLU\n",
      "\n",
      "Finished training N1024_L4_PReLU, Test Loss=0.004352, Training Time = 4m 33s, Epochs = 41\n",
      "\n",
      "Model 53, Epoch 23/210, Train Loss=0.004877, Test Loss=0.004143, Elapsed: 68m 54s\n",
      "Early stopping triggered at epoch 24 for N1024_L4_Sigmoid\n",
      "\n",
      "Finished training N1024_L4_Sigmoid, Test Loss=0.004143, Training Time = 2m 37s, Epochs = 24\n",
      "\n",
      "Model 54, Epoch 22/210, Train Loss=0.003577, Test Loss=0.004596, Elapsed: 71m 26s\n",
      "Early stopping triggered at epoch 23 for N1024_L4_Tanh\n",
      "\n",
      "Finished training N1024_L4_Tanh, Test Loss=0.006040, Training Time = 2m 31s, Epochs = 23\n",
      "\n",
      "Model 55, Epoch 61/240, Train Loss=0.000389, Test Loss=0.000799, Elapsed: 83m 49s\n",
      "Early stopping triggered at epoch 62 for N1024_L5_ReLU\n",
      "\n",
      "Finished training N1024_L5_ReLU, Test Loss=0.000814, Training Time = 12m 27s, Epochs = 62\n",
      "\n",
      "Model 56, Epoch 20/240, Train Loss=0.003970, Test Loss=0.006053, Elapsed: 86m 50s5s\n",
      "Early stopping triggered at epoch 21 for N1024_L5_LeakyReLU\n",
      "\n",
      "Finished training N1024_L5_LeakyReLU, Test Loss=0.005629, Training Time = 2m 57s, Epochs = 21\n",
      "\n",
      "Model 57, Epoch 22/240, Train Loss=0.004954, Test Loss=0.004189, Elapsed: 90m 8sss\n",
      "Early stopping triggered at epoch 23 for N1024_L5_PReLU\n",
      "\n",
      "Finished training N1024_L5_PReLU, Test Loss=0.004175, Training Time = 3m 18s, Epochs = 23\n",
      "\n",
      "Model 58, Epoch 23/240, Train Loss=0.004877, Test Loss=0.004143, Elapsed: 93m 30s\n",
      "Early stopping triggered at epoch 24 for N1024_L5_Sigmoid\n",
      "\n",
      "Finished training N1024_L5_Sigmoid, Test Loss=0.004143, Training Time = 3m 21s, Epochs = 24\n",
      "\n",
      "Model 59, Epoch 24/240, Train Loss=0.005289, Test Loss=0.011504, Elapsed: 96m 59s\n",
      "Early stopping triggered at epoch 25 for N1024_L5_Tanh\n",
      "\n",
      "Finished training N1024_L5_Tanh, Test Loss=0.012917, Training Time = 3m 28s, Epochs = 25\n",
      "\n",
      "Best model: N256_L3_ReLU, Test Loss = 0.000126\n"
     ]
    }
   ],
   "source": [
    "# EarlyStopping helper\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=1e-4):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.counter = 0\n",
    "        self.should_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.should_stop = True\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Example dictionaries containing datasets\n",
    "datasets = {\n",
    "    256: (f_data_256, F_data_256),\n",
    "    512: (f_data_512, F_data_512),\n",
    "    1024: (f_data_1024, F_data_1024)\n",
    "}\n",
    "\n",
    "patience_vals = {\n",
    "    256: 5,\n",
    "    512: 10,\n",
    "    1024: 20\n",
    "}\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "learning_rate = 1e-3\n",
    "i = 0\n",
    "\n",
    "best_model_name = None\n",
    "lowest_test_loss = float('inf')\n",
    "best_model_state = None\n",
    "start_time = time.time()\n",
    "model_num = -1\n",
    "\n",
    "for N in [256, 512, 1024]:\n",
    "    f_data, F_data = datasets[N]\n",
    "    epochs += 10 * i\n",
    "    train_ds, test_ds, train_loader, test_loader = data_loaders(f_data, F_data)\n",
    "    j = 0\n",
    "    \n",
    "    for layers in [2, 3, 4, 5]:\n",
    "        epochs += 10 * j\n",
    "        for act in ['ReLU', 'LeakyReLU', 'PReLU', 'Sigmoid', 'Tanh']:\n",
    "            model_num += 1\n",
    "            model_name = f\"N{N}_L{layers}_{act}\"\n",
    "            model = models[model_name].to(device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            criterion = nn.MSELoss()\n",
    "            \n",
    "            early_stopper = EarlyStopping(patience=patience_vals[N], min_delta=1e-4)\n",
    "            model_start_time = time.time()\n",
    "            epoch = 0\n",
    "            for epoch in range(epochs):\n",
    "                model.train()\n",
    "                total_loss = 0\n",
    "                for xb, yb in train_loader:\n",
    "                    xb, yb = xb.to(device), yb.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    pred = model(xb)\n",
    "                    loss = criterion(pred, yb)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item() * xb.size(0)\n",
    "                avg_loss = total_loss / len(train_loader.dataset)\n",
    "                \n",
    "                # Compute test/validation loss\n",
    "                model.eval()\n",
    "                total_test_loss = 0\n",
    "                with torch.no_grad():\n",
    "                    for xb, yb in test_loader:\n",
    "                        xb, yb = xb.to(device), yb.to(device)\n",
    "                        pred = model(xb)\n",
    "                        loss = criterion(pred, yb)\n",
    "                        total_test_loss += loss.item() * xb.size(0)\n",
    "                test_loss = total_test_loss / len(test_loader.dataset)\n",
    "                \n",
    "                # Early stopping check\n",
    "                early_stopper(test_loss)\n",
    "                if early_stopper.should_stop:\n",
    "                    print(f\"\\nEarly stopping triggered at epoch {epoch+1} for {model_name}\")\n",
    "                    break\n",
    "                \n",
    "                elapsed = time.time() - start_time\n",
    "                mins, secs = divmod(int(elapsed), 60)\n",
    "                print(f\"\\rModel {model_num}, Epoch {epoch+1}/{epochs}, \"\n",
    "                      f\"Train Loss={avg_loss:.6f}, Test Loss={test_loss:.6f}, \"\n",
    "                      f\"Elapsed: {mins}m {secs}s\", end='', flush=True)\n",
    "            \n",
    "            # Record training time\n",
    "            train_time = time.time() - model_start_time\n",
    "            mins, secs = divmod(int(train_time), 60)\n",
    "            print()\n",
    "            \n",
    "            # Save metrics\n",
    "            metrics[model_name][\"Train Loss\"] = avg_loss\n",
    "            metrics[model_name][\"Test Loss\"] = test_loss\n",
    "            metrics[model_name][\"Training Time\"] = train_time\n",
    "            metrics[model_name][\"Epochs\"] = epoch + 1\n",
    "            \n",
    "            # Update best model\n",
    "            if test_loss < lowest_test_loss:\n",
    "                lowest_test_loss = test_loss\n",
    "                best_model_name = model_name\n",
    "                best_model_state = model.state_dict()\n",
    "            \n",
    "            print(f\"Finished training {model_name}, Test Loss={test_loss:.6f}, Training Time = {mins}m {secs}s, Epochs = {epoch + 1}\\n\")\n",
    "        j += 1\n",
    "    i += 1\n",
    "\n",
    "print(f\"Best model: {best_model_name}, Test Loss = {lowest_test_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "73ed567f-b5b8-451f-810b-9c2c033aea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    torch.save(model.state_dict(), f\"{name}.pth\")\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
